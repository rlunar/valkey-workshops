# Semantic Search SQL Cache

Intelligent caching system that uses embeddings and vector similarity to reduce LLM calls for SQL generation.

## How It Works

The system uses **semantic similarity** to detect when a new query is similar to a previously seen query, avoiding expensive LLM calls:

1. **Embedding Generation**: Each natural language prompt is converted to a vector embedding
2. **Vector Search**: New prompts are compared against cached embeddings using cosine similarity
3. **Smart Caching**: If similarity exceeds threshold (default 0.85), returns cached SQL
4. **Fallback**: If no similar query found, generates new SQL with LLM and caches it

### Example

These queries are semantically similar and will hit the cache:
- "Flight manifest - all passengers on a specific flight 115"
- "Give me the passenger details from flight 115"
- "Show me all passengers on flight 115"

## Key Structure in Valkey/Redis

```
semantic:prompt:<hash>     ‚Üí db:query:<hash2>     (maps prompt to query result)
db:query:<hash2>           ‚Üí {sql, time, tokens}  (NLP result from LLM)
db:cache:<hash2>           ‚Üí <query result>       (SQL execution result)
embedding:prompt:<hash>    ‚Üí <embedding vector>   (for vector search)
```

Where:
- `<hash>` = SHA1 of the prompt text
- `<hash2>` = SHA1 of the SQL statement

## Prerequisites

```bash
# Install dependencies (already in pyproject.toml)
uv sync

# First run will download the embedding model (~90MB)
# This happens automatically on first use

# Ensure Ollama is running
ollama serve

# Ensure Valkey/Redis is running
redis-server
# or
valkey-server
```

**Note**: On first run, the system will download the `all-MiniLM-L6-v2` embedding model (~90MB). This is a one-time download and will be cached locally.

## Usage

### Demo Mode (Default)

Shows how semantic caching works with similar queries:

```bash
cd samples
uv run python semantic_search.py
```

### Interactive Mode

Enter your own queries:

```bash
uv run python semantic_search.py --mode interactive
```

### With Different Model

```bash
# Use better model for SQL generation
uv run python semantic_search.py --model llama3.2

# Use different similarity threshold
uv run python semantic_search.py --threshold 0.90
```

### Clear Cache

```bash
uv run python semantic_search.py --clear
```

## Configuration Options

```bash
python semantic_search.py \
  --host localhost \          # Redis/Valkey host
  --port 6379 \              # Redis/Valkey port
  --model llama3.2 \         # Ollama model for SQL generation
  --threshold 0.85 \         # Similarity threshold (0-1)
  --mode demo \              # demo or interactive
  --clear                    # Clear cache before starting
```

## Example Output

```
============================================================
SEMANTIC SEARCH DEMO - Testing cache with similar queries
============================================================

1. Query: Flight manifest - all passengers on a specific flight 115
------------------------------------------------------------
üîç Searching for similar prompts...
ü§ñ No similar prompt found. Generating new SQL with LLM...
üíæ Cached result for future queries

Generated SQL:
SELECT b.seat, p.firstname, p.lastname, p.passportno, pd.country, b.price 
FROM booking b 
INNER JOIN passenger p ON b.passenger_id = p.passenger_id 
LEFT JOIN passengerdetails pd ON p.passenger_id = pd.passenger_id 
WHERE b.flight_id = 115 
ORDER BY b.seat ASC;

ü§ñ NEW QUERY - Generated by LLM
   ‚è±Ô∏è  Generation time: 3.45s
   üî¢ Tokens: 1678


2. Query: Give me the passenger details from flight 115
------------------------------------------------------------
üîç Searching for similar prompts...
‚ú® Found similar prompt (similarity: 0.912)
   Original: Flight manifest - all passengers on a specific flight 115
   Current:  Give me the passenger details from flight 115

Generated SQL:
SELECT b.seat, p.firstname, p.lastname, p.passportno, pd.country, b.price 
FROM booking b 
INNER JOIN passenger p ON b.passenger_id = p.passenger_id 
LEFT JOIN passengerdetails pd ON p.passenger_id = pd.passenger_id 
WHERE b.flight_id = 115 
ORDER BY b.seat ASC;

‚ú® SEMANTIC CACHE HIT (similarity: 0.912)
   Similar to: Flight manifest - all passengers on a specific flight 115
   ‚ö° Lookup time: 0.023s (saved ~3.45s)


============================================================
üìà SUMMARY:
   Total queries: 7
   Cache hits: 4 (57.1%)
   New generations: 3
   Total LLM time: 8.92s
   Average per new query: 2.97s

üìä CACHE STATS:
   Cached prompts: 4
   Cached queries: 4
   Embeddings stored: 4
============================================================
```

## Performance Benefits

### Without Semantic Cache
- Every query requires LLM call (~2-5 seconds)
- High token usage
- Expensive API costs

### With Semantic Cache
- Similar queries return instantly (~0.02 seconds)
- 100x+ faster for cached queries
- Significant cost savings
- Better user experience

## Similarity Threshold

The `--threshold` parameter controls how similar queries need to be:

- **0.80+**: Very strict, only nearly identical queries match
- **0.70-0.80**: Recommended (default 0.70), catches paraphrases and variations
- **0.60-0.70**: Looser, may match less similar queries
- **<0.60**: Too loose, may return incorrect results

Based on testing with `all-MiniLM-L6-v2`:
- Similar queries about the same topic: 0.65-0.80
- Paraphrases: 0.70-0.85
- Different topics: <0.40

## Interactive Commands

In interactive mode:
- `stats` - Show cache statistics
- `clear` - Clear all cached data
- `quit` or `q` - Exit

## Architecture

```
User Query
    ‚Üì
Generate Embedding (sentence-transformers)
    ‚Üì
Check Exact Match (semantic:prompt:<hash>)
    ‚Üì (if not found)
Vector Search (FT.SEARCH with KNN)
    ‚Üì
Similarity > Threshold?
    ‚Üì YES                    ‚Üì NO
Return Cached SQL    Generate New SQL (Ollama)
                            ‚Üì
                     Cache Result + Embedding
```

## Embedding Model

Uses `all-MiniLM-L6-v2` by default:
- 384-dimensional vectors
- Fast inference (~0.01s per query)
- Good balance of speed and accuracy
- Runs locally, no API calls

## Vector Search

Uses Valkey/Redis Vector Search (RediSearch):
- HNSW algorithm for fast approximate nearest neighbor search
- Cosine distance metric
- Sub-millisecond search times
- Scales to millions of vectors

## Use Cases

1. **Chatbots**: Users ask similar questions in different ways
2. **Analytics Dashboards**: Common queries with slight variations
3. **Data Exploration**: Iterative querying with refinements
4. **Multi-user Systems**: Different users asking similar questions

## Limitations

- Requires Valkey/Redis with RediSearch module
- Embedding generation adds small overhead (~10-20ms)
- Cache effectiveness depends on query patterns
- May return cached results for slightly different intents

## Troubleshooting

**First run takes a long time:**
- The embedding model (~90MB) is being downloaded
- This is a one-time operation
- Subsequent runs will be fast

**Vector search not working:**
```bash
# Check if RediSearch module is loaded
redis-cli MODULE LIST

# If not loaded, you may need Redis Stack or Valkey with search module
# For testing without vector search, the system will fall back to exact matching
```

**Poor cache hit rate:**
- Lower the similarity threshold
- Check if queries are truly similar
- Verify embedding model is loaded correctly

**Slow performance:**
- Check Redis/Valkey connection
- Ensure embedding model is cached in memory
- Consider using GPU for embeddings (if available)

**Module not found errors:**
```bash
# Ensure all dependencies are installed
uv sync

# Or manually install
pip install sentence-transformers numpy
```
